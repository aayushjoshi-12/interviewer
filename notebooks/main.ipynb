{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# TODO: improve all prompts\n",
    "# TODO: the model is going to get deprecated soon so look for alternates that perform well. \n",
    "llm = ChatGroq(model=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "def parse_pdf(path: str) -> str:\n",
    "    docs = PyPDFLoader(path).load()\n",
    "    text = \" \".join(doc.page_content for doc in docs)\n",
    "    return text\n",
    "\n",
    "\n",
    "class Experience(BaseModel):\n",
    "    \"\"\"Schema for an experience entry on resume.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"The title of the job.\")\n",
    "    description: str = Field(description=\"A description of the job.\")\n",
    "\n",
    "\n",
    "class Project(BaseModel):\n",
    "    \"\"\"Schema for a project entry on resume.\"\"\"\n",
    "\n",
    "    title: str = Field(description=\"The title of the project.\")\n",
    "    description: str = Field(description=\"A description of the project.\")\n",
    "\n",
    "\n",
    "class Resume(BaseModel):\n",
    "    \"\"\"Breakdown of a resume into experiences, projects, and skills.\"\"\"\n",
    "\n",
    "    experiences: List[Experience] = Field(\n",
    "        description=\"The experiences listed on the resume.\"\n",
    "    )\n",
    "    projects: List[Project] = Field(description=\"The projects listed on the resume.\")\n",
    "    skills: List[str] = Field(description=\"The skills listed on the resume.\")\n",
    "\n",
    "\n",
    "resume_breaker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a resume parser. Your task is to breakdown the resume provided \"\n",
    "            \"below into three sections: experiences, projects, and skills.\",\n",
    "        ),\n",
    "        (\"system\", \"<resume>\\n{resume}\\n</resume>\"),\n",
    "    ]\n",
    ")\n",
    "resume_breaker = resume_breaker_prompt | llm.with_structured_output(Resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume = resume_breaker.invoke({\"resume\": parse_pdf(\"./media/resume.pdf\")})\n",
    "# resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodingQuestions(BaseModel):\n",
    "    \"\"\"Schema for a coding question.\"\"\"\n",
    "\n",
    "    questions: List[str] = Field(description=\"The list of technical coding questions.\")\n",
    "\n",
    "\n",
    "# TODO: use few shot prompts with this system prompt and differnt techstack and questions related to it.\n",
    "# You can use job description as well if given in the state.\n",
    "generate_coding_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a coding interviewer. Your task is to generate exactly 5 technical \"\n",
    "            \"programming/coding questions based on the skills mentioned below.\"\n",
    "        ),\n",
    "        (\"system\", \"<skills>\\n{skills}\\n</skills>\"),\n",
    "    ]\n",
    ")\n",
    "generate_coding_question = generate_coding_question_prompt | llm.with_structured_output(\n",
    "    CodingQuestions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skills = \", \".join(skill for skill in resume.skills)\n",
    "# coding_questions = generate_coding_question.invoke({\"skills\": skills})\n",
    "# coding_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodingInterviewScore(BaseModel):\n",
    "    \"\"\"Schema for a coding interview score.\"\"\"\n",
    "\n",
    "    score: int = Field(description=\"The score of the candidate's response.\")\n",
    "\n",
    "\n",
    "coding_question_assessment_promt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a coding interviewer. Your task is to assess the candidate's \"\n",
    "            \"response by a score between 1 to 10 where 1 is very bad and 10 is perfect \"\n",
    "            \"answer. Here is the question: \\n{question}\",\n",
    "        ),\n",
    "        (\"user\", \"{response}\"),\n",
    "    ]\n",
    ")\n",
    "coding_question_assessment = (\n",
    "    coding_question_assessment_promt | llm.with_structured_output(CodingInterviewScore)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class ExperienceInterviewScore(BaseModel):\n",
    "    \"\"\"Schema for an experience interview score.\"\"\"\n",
    "\n",
    "    score: int = Field(\n",
    "        description=(\n",
    "            \"The score of the candidate's responses between 1 to 10 where 1 is \"\n",
    "            \"extremely bad and 10 is perfect.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class ExperienceInterviewQuestion(BaseModel):\n",
    "    \"\"\"Schema for an experience interview.\"\"\"\n",
    "\n",
    "    response: Union[str, ExperienceInterviewScore] = Field(\n",
    "    # response: str = Field(\n",
    "        description=(\n",
    "            \"The question you want to ask. If you want to stop the conversation, \"\n",
    "            \"provide the score to candidate's responses using the Experience \"\n",
    "            \"Interview score schema.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "expereince_interviewer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an highly experienced interviewer at a tech company and your task is to take \"\n",
    "            \"interviews of new candidatew for the provided job description (if any is provided). You \"\n",
    "            \"can ask questions about the candidate's previous work experiences. If you think that you \"\n",
    "            \"need to ask a follow up question, you can ask that as well or else you can move on to the \"\n",
    "            \"next question. No need to extend you conversation. Ask only relevant questions. Do not \"\n",
    "            \"call any tools\",\n",
    "        ),\n",
    "        (\"system\", \"<experience>\\n{experience}\\n</experience>\"),\n",
    "        (\"system\", \"<projects>\\n{projects}\\n</projects>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "experience_interviewer = expereince_interviewer_prompt | llm.with_structured_output(\n",
    "    ExperienceInterviewQuestion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience_interviewer.invoke(\n",
    "#     {\n",
    "#         \"experience\": resume.experiences,\n",
    "#         \"projects\": resume.projects,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class Interview(MessagesState):\n",
    "    job_description: str\n",
    "    experiences: List[Experience]\n",
    "    projects: List[Project]\n",
    "    skills: List[str]\n",
    "    coding_interview_score: int\n",
    "    experience_interview_score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "async def resume_parser_node(state: Interview):\n",
    "    resume = parse_pdf(\n",
    "        \"../media/resume.pdf\"\n",
    "    )  # TODO: we can make it a task using @task in the node ig.\n",
    "    parsed_resume = await resume_breaker.ainvoke({\"resume\": resume})\n",
    "    return {\n",
    "        \"experiences\": parsed_resume.experiences,\n",
    "        \"projects\": parsed_resume.projects,\n",
    "        \"skills\": parsed_resume.skills,\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO: implement it like the experience_interviewer_node if that works\n",
    "async def coding_interviewer_node(state: Interview):\n",
    "    skills = \", \".join(skill for skill in state[\"skills\"])\n",
    "    coding_questions = await generate_coding_question.ainvoke({\"skills\": skills})\n",
    "    questions = coding_questions.questions\n",
    "    total_score = 10\n",
    "    # TODO: uncomment it\n",
    "    for question in questions:\n",
    "        answer = interrupt(value=f\"Answer the following: {question}\\n\")\n",
    "        score = await coding_question_assessment.ainvoke(\n",
    "            {\"question\": question, \"response\": answer}\n",
    "        )\n",
    "        total_score += score.score\n",
    "    return {\n",
    "        \"coding_interview_score\": total_score // len(questions),\n",
    "    }\n",
    "\n",
    "\n",
    "async def experience_interviewer_node(state: Interview):\n",
    "    response = await experience_interviewer.ainvoke(\n",
    "        {\"experience\": state[\"experiences\"], \"projects\": state[\"projects\"]}\n",
    "    )\n",
    "    if isinstance(response, ExperienceInterviewScore):\n",
    "        return {\"experience_interview_score\": response.score}\n",
    "    return Command(\n",
    "        update={\"messages\": [{\"role\": \"ai\", \"content\": response.response}]},\n",
    "        goto=\"candidate_node\",\n",
    "    )\n",
    "\n",
    "\n",
    "def candidate_node(state: Interview):\n",
    "    answer = interrupt(\"Answer the question. \\n\")\n",
    "    return Command(\n",
    "        update={\"messages\": [{\"role\": \"human\", \"content\": answer}]},\n",
    "        goto=\"experience_interviewer_node\",\n",
    "    )\n",
    "\n",
    "\n",
    "def should_end(state: Interview):\n",
    "    if \"experience_interview_score\" in state and state[\"experience_interview_score\"]:\n",
    "        return END\n",
    "    else:\n",
    "        return \"candidate_node\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "workflow = StateGraph(Interview)\n",
    "\n",
    "workflow.add_node(\"resume_parser_node\", resume_parser_node)\n",
    "workflow.add_node(\"coding_interviewer_node\", coding_interviewer_node)\n",
    "workflow.add_node(\"experience_interviewer_node\", experience_interviewer_node)\n",
    "workflow.add_node(\"candidate_node\", candidate_node)\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"resume_parser_node\")\n",
    "workflow.add_edge(\"resume_parser_node\", \"coding_interviewer_node\")\n",
    "workflow.add_edge(\"coding_interviewer_node\", \"experience_interviewer_node\")\n",
    "workflow.add_edge(\"candidate_node\", \"experience_interviewer_node\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"experience_interviewer_node\", should_end, [\"candidate_node\", END]\n",
    ")\n",
    "\n",
    "graph = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[-1:checkpoint]\u001b[0m \u001b[1mState at the end of step -1:\n",
      "\u001b[0m{'messages': []}\n",
      "\u001b[36;1m\u001b[1;3m[0:tasks]\u001b[0m \u001b[1mStarting 1 task for step 0:\n",
      "\u001b[0m- \u001b[32;1m\u001b[1;3m__start__\u001b[0m -> {'job_description': '\\n'\n",
      "                    'We are seeking a highly skilled AI Developer with '\n",
      "                    'expertise in Llama, OpenAI, and Falcon models to join our '\n",
      "                    'team. The ideal candidate will have strong experience in '\n",
      "                    'developing, fine-tuning, and deploying AI models, working '\n",
      "                    'with large language models (LLMs), and integrating AI '\n",
      "                    'solutions into real-world applications.\\n'\n",
      "                    '\\n'\n",
      "                    'Key Responsibilities:\\n'\n",
      "                    '• Develop, fine-tune, and deploy AI/ML models using '\n",
      "                    'Llama, OpenAI, and Falcon.\\n'\n",
      "                    '• Optimize and customize pre-trained language models for '\n",
      "                    'specific use cases.\\n'\n",
      "                    '• Implement AI-powered solutions for chatbots, '\n",
      "                    'automation, and text generation.\\n'\n",
      "                    '• Integrate AI models into web and cloud-based '\n",
      "                    'applications.\\n'\n",
      "                    '• Work with APIs and SDKs from OpenAI, Meta (Llama), and '\n",
      "                    'Falcon to build scalable AI solutions.\\n'\n",
      "                    '• Perform data preprocessing, model training, and '\n",
      "                    'performance evaluation.\\n'\n",
      "                    '• Stay up to date with the latest advancements in natural '\n",
      "                    'language processing (NLP) and LLMs.\\n'\n",
      "                    '• Collaborate with cross-functional teams to deliver '\n",
      "                    'AI-driven projects.\\n'\n",
      "                    '• Troubleshoot and enhance model performance using '\n",
      "                    'hyperparameter tuning and optimization techniques.\\n'\n",
      "                    '• Ensure compliance with AI ethics, security, and data '\n",
      "                    'privacy standards.\\n'\n",
      "                    '\\n'\n",
      "                    'Required Skills & Qualifications:\\n'\n",
      "                    '• Experience between 3-6 yrs in AI/ML development, with '\n",
      "                    'hands-on experience in Llama, OpenAI (GPT models), and '\n",
      "                    'Falcon.\\n'\n",
      "                    '• Strong programming skills in Python, with experience in '\n",
      "                    'TensorFlow, PyTorch, or Hugging Face Transformers.\\n'\n",
      "                    '• Proficiency in API development and integration with AI '\n",
      "                    'models.\\n'\n",
      "                    '• Experience in fine-tuning large language models (LLMs) '\n",
      "                    'for various applications.\\n'\n",
      "                    '• Knowledge of cloud platforms (AWS, GCP, or Azure) for '\n",
      "                    'AI model deployment.\\n'\n",
      "                    '• Hands-on experience with NLP techniques, embeddings, '\n",
      "                    'and tokenization.\\n'\n",
      "                    '• Strong understanding of vector databases (e.g., '\n",
      "                    'Pinecone, FAISS) for efficient AI retrieval tasks.\\n'\n",
      "                    '• Experience in optimizing AI models for speed, accuracy, '\n",
      "                    'and scalability.\\n'\n",
      "                    '• Familiarity with Docker and Kubernetes for deploying AI '\n",
      "                    'applications.\\n'\n",
      "                    '• Strong problem-solving skills and ability to work in an '\n",
      "                    'agile development environment.\\n'\n",
      "                    '\\n'\n",
      "                    'Preferred Qualifications:\\n'\n",
      "                    '• Experience working with LangChain and RAG-based '\n",
      "                    '(Retrieval-Augmented Generation) AI models.\\n'\n",
      "                    '• Knowledge of MLOps practices for model lifecycle '\n",
      "                    'management.\\n'\n",
      "                    '• Experience with Graph Neural Networks (GNNs) or '\n",
      "                    'multimodal AI models.\\n'\n",
      "                    '• Background in AI security and bias mitigation.'}\n",
      "\u001b[36;1m\u001b[1;3m[0:writes]\u001b[0m \u001b[1mFinished step 0 with writes to 1 channel:\n",
      "\u001b[0m- \u001b[33;1m\u001b[1;3mjob_description\u001b[0m -> ('\\n'\n",
      " 'We are seeking a highly skilled AI Developer with expertise in Llama, '\n",
      " 'OpenAI, and Falcon models to join our team. The ideal candidate will have '\n",
      " 'strong experience in developing, fine-tuning, and deploying AI models, '\n",
      " 'working with large language models (LLMs), and integrating AI solutions into '\n",
      " 'real-world applications.\\n'\n",
      " '\\n'\n",
      " 'Key Responsibilities:\\n'\n",
      " '• Develop, fine-tune, and deploy AI/ML models using Llama, OpenAI, and '\n",
      " 'Falcon.\\n'\n",
      " '• Optimize and customize pre-trained language models for specific use '\n",
      " 'cases.\\n'\n",
      " '• Implement AI-powered solutions for chatbots, automation, and text '\n",
      " 'generation.\\n'\n",
      " '• Integrate AI models into web and cloud-based applications.\\n'\n",
      " '• Work with APIs and SDKs from OpenAI, Meta (Llama), and Falcon to build '\n",
      " 'scalable AI solutions.\\n'\n",
      " '• Perform data preprocessing, model training, and performance evaluation.\\n'\n",
      " '• Stay up to date with the latest advancements in natural language '\n",
      " 'processing (NLP) and LLMs.\\n'\n",
      " '• Collaborate with cross-functional teams to deliver AI-driven projects.\\n'\n",
      " '• Troubleshoot and enhance model performance using hyperparameter tuning and '\n",
      " 'optimization techniques.\\n'\n",
      " '• Ensure compliance with AI ethics, security, and data privacy standards.\\n'\n",
      " '\\n'\n",
      " 'Required Skills & Qualifications:\\n'\n",
      " '• Experience between 3-6 yrs in AI/ML development, with hands-on experience '\n",
      " 'in Llama, OpenAI (GPT models), and Falcon.\\n'\n",
      " '• Strong programming skills in Python, with experience in TensorFlow, '\n",
      " 'PyTorch, or Hugging Face Transformers.\\n'\n",
      " '• Proficiency in API development and integration with AI models.\\n'\n",
      " '• Experience in fine-tuning large language models (LLMs) for various '\n",
      " 'applications.\\n'\n",
      " '• Knowledge of cloud platforms (AWS, GCP, or Azure) for AI model '\n",
      " 'deployment.\\n'\n",
      " '• Hands-on experience with NLP techniques, embeddings, and tokenization.\\n'\n",
      " '• Strong understanding of vector databases (e.g., Pinecone, FAISS) for '\n",
      " 'efficient AI retrieval tasks.\\n'\n",
      " '• Experience in optimizing AI models for speed, accuracy, and scalability.\\n'\n",
      " '• Familiarity with Docker and Kubernetes for deploying AI applications.\\n'\n",
      " '• Strong problem-solving skills and ability to work in an agile development '\n",
      " 'environment.\\n'\n",
      " '\\n'\n",
      " 'Preferred Qualifications:\\n'\n",
      " '• Experience working with LangChain and RAG-based (Retrieval-Augmented '\n",
      " 'Generation) AI models.\\n'\n",
      " '• Knowledge of MLOps practices for model lifecycle management.\\n'\n",
      " '• Experience with Graph Neural Networks (GNNs) or multimodal AI models.\\n'\n",
      " '• Background in AI security and bias mitigation.')\n",
      "\u001b[36;1m\u001b[1;3m[0:checkpoint]\u001b[0m \u001b[1mState at the end of step 0:\n",
      "\u001b[0m{'job_description': '\\n'\n",
      "                    'We are seeking a highly skilled AI Developer with '\n",
      "                    'expertise in Llama, OpenAI, and Falcon models to join our '\n",
      "                    'team. The ideal candidate will have strong experience in '\n",
      "                    'developing, fine-tuning, and deploying AI models, working '\n",
      "                    'with large language models (LLMs), and integrating AI '\n",
      "                    'solutions into real-world applications.\\n'\n",
      "                    '\\n'\n",
      "                    'Key Responsibilities:\\n'\n",
      "                    '• Develop, fine-tune, and deploy AI/ML models using '\n",
      "                    'Llama, OpenAI, and Falcon.\\n'\n",
      "                    '• Optimize and customize pre-trained language models for '\n",
      "                    'specific use cases.\\n'\n",
      "                    '• Implement AI-powered solutions for chatbots, '\n",
      "                    'automation, and text generation.\\n'\n",
      "                    '• Integrate AI models into web and cloud-based '\n",
      "                    'applications.\\n'\n",
      "                    '• Work with APIs and SDKs from OpenAI, Meta (Llama), and '\n",
      "                    'Falcon to build scalable AI solutions.\\n'\n",
      "                    '• Perform data preprocessing, model training, and '\n",
      "                    'performance evaluation.\\n'\n",
      "                    '• Stay up to date with the latest advancements in natural '\n",
      "                    'language processing (NLP) and LLMs.\\n'\n",
      "                    '• Collaborate with cross-functional teams to deliver '\n",
      "                    'AI-driven projects.\\n'\n",
      "                    '• Troubleshoot and enhance model performance using '\n",
      "                    'hyperparameter tuning and optimization techniques.\\n'\n",
      "                    '• Ensure compliance with AI ethics, security, and data '\n",
      "                    'privacy standards.\\n'\n",
      "                    '\\n'\n",
      "                    'Required Skills & Qualifications:\\n'\n",
      "                    '• Experience between 3-6 yrs in AI/ML development, with '\n",
      "                    'hands-on experience in Llama, OpenAI (GPT models), and '\n",
      "                    'Falcon.\\n'\n",
      "                    '• Strong programming skills in Python, with experience in '\n",
      "                    'TensorFlow, PyTorch, or Hugging Face Transformers.\\n'\n",
      "                    '• Proficiency in API development and integration with AI '\n",
      "                    'models.\\n'\n",
      "                    '• Experience in fine-tuning large language models (LLMs) '\n",
      "                    'for various applications.\\n'\n",
      "                    '• Knowledge of cloud platforms (AWS, GCP, or Azure) for '\n",
      "                    'AI model deployment.\\n'\n",
      "                    '• Hands-on experience with NLP techniques, embeddings, '\n",
      "                    'and tokenization.\\n'\n",
      "                    '• Strong understanding of vector databases (e.g., '\n",
      "                    'Pinecone, FAISS) for efficient AI retrieval tasks.\\n'\n",
      "                    '• Experience in optimizing AI models for speed, accuracy, '\n",
      "                    'and scalability.\\n'\n",
      "                    '• Familiarity with Docker and Kubernetes for deploying AI '\n",
      "                    'applications.\\n'\n",
      "                    '• Strong problem-solving skills and ability to work in an '\n",
      "                    'agile development environment.\\n'\n",
      "                    '\\n'\n",
      "                    'Preferred Qualifications:\\n'\n",
      "                    '• Experience working with LangChain and RAG-based '\n",
      "                    '(Retrieval-Augmented Generation) AI models.\\n'\n",
      "                    '• Knowledge of MLOps practices for model lifecycle '\n",
      "                    'management.\\n'\n",
      "                    '• Experience with Graph Neural Networks (GNNs) or '\n",
      "                    'multimodal AI models.\\n'\n",
      "                    '• Background in AI security and bias mitigation.',\n",
      " 'messages': []}\n",
      "\u001b[36;1m\u001b[1;3m[1:tasks]\u001b[0m \u001b[1mStarting 1 task for step 1:\n",
      "\u001b[0m- \u001b[32;1m\u001b[1;3mresume_parser_node\u001b[0m -> {'job_description': '\\n'\n",
      "                    'We are seeking a highly skilled AI Developer with '\n",
      "                    'expertise in Llama, OpenAI, and Falcon models to join our '\n",
      "                    'team. The ideal candidate will have strong experience in '\n",
      "                    'developing, fine-tuning, and deploying AI models, working '\n",
      "                    'with large language models (LLMs), and integrating AI '\n",
      "                    'solutions into real-world applications.\\n'\n",
      "                    '\\n'\n",
      "                    'Key Responsibilities:\\n'\n",
      "                    '• Develop, fine-tune, and deploy AI/ML models using '\n",
      "                    'Llama, OpenAI, and Falcon.\\n'\n",
      "                    '• Optimize and customize pre-trained language models for '\n",
      "                    'specific use cases.\\n'\n",
      "                    '• Implement AI-powered solutions for chatbots, '\n",
      "                    'automation, and text generation.\\n'\n",
      "                    '• Integrate AI models into web and cloud-based '\n",
      "                    'applications.\\n'\n",
      "                    '• Work with APIs and SDKs from OpenAI, Meta (Llama), and '\n",
      "                    'Falcon to build scalable AI solutions.\\n'\n",
      "                    '• Perform data preprocessing, model training, and '\n",
      "                    'performance evaluation.\\n'\n",
      "                    '• Stay up to date with the latest advancements in natural '\n",
      "                    'language processing (NLP) and LLMs.\\n'\n",
      "                    '• Collaborate with cross-functional teams to deliver '\n",
      "                    'AI-driven projects.\\n'\n",
      "                    '• Troubleshoot and enhance model performance using '\n",
      "                    'hyperparameter tuning and optimization techniques.\\n'\n",
      "                    '• Ensure compliance with AI ethics, security, and data '\n",
      "                    'privacy standards.\\n'\n",
      "                    '\\n'\n",
      "                    'Required Skills & Qualifications:\\n'\n",
      "                    '• Experience between 3-6 yrs in AI/ML development, with '\n",
      "                    'hands-on experience in Llama, OpenAI (GPT models), and '\n",
      "                    'Falcon.\\n'\n",
      "                    '• Strong programming skills in Python, with experience in '\n",
      "                    'TensorFlow, PyTorch, or Hugging Face Transformers.\\n'\n",
      "                    '• Proficiency in API development and integration with AI '\n",
      "                    'models.\\n'\n",
      "                    '• Experience in fine-tuning large language models (LLMs) '\n",
      "                    'for various applications.\\n'\n",
      "                    '• Knowledge of cloud platforms (AWS, GCP, or Azure) for '\n",
      "                    'AI model deployment.\\n'\n",
      "                    '• Hands-on experience with NLP techniques, embeddings, '\n",
      "                    'and tokenization.\\n'\n",
      "                    '• Strong understanding of vector databases (e.g., '\n",
      "                    'Pinecone, FAISS) for efficient AI retrieval tasks.\\n'\n",
      "                    '• Experience in optimizing AI models for speed, accuracy, '\n",
      "                    'and scalability.\\n'\n",
      "                    '• Familiarity with Docker and Kubernetes for deploying AI '\n",
      "                    'applications.\\n'\n",
      "                    '• Strong problem-solving skills and ability to work in an '\n",
      "                    'agile development environment.\\n'\n",
      "                    '\\n'\n",
      "                    'Preferred Qualifications:\\n'\n",
      "                    '• Experience working with LangChain and RAG-based '\n",
      "                    '(Retrieval-Augmented Generation) AI models.\\n'\n",
      "                    '• Knowledge of MLOps practices for model lifecycle '\n",
      "                    'management.\\n'\n",
      "                    '• Experience with Graph Neural Networks (GNNs) or '\n",
      "                    'multimodal AI models.\\n'\n",
      "                    '• Background in AI security and bias mitigation.',\n",
      " 'messages': []}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33maayush1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m graph.astream(\n\u001b[32m      4\u001b[39m     {\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mjob_description\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mWe are seeking a highly skilled AI Developer with expertise in Llama, OpenAI, and Falcon models to join our team. The ideal candidate will have strong experience in developing, fine-tuning, and deploying AI models, working with large language models (LLMs), and integrating AI solutions into real-world applications.\u001b[39m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[33mKey Responsibilities:\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m• Develop, fine-tune, and deploy AI/ML models using Llama, OpenAI, and Falcon.\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m• Optimize and customize pre-trained language models for specific use cases.\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m• Implement AI-powered solutions for chatbots, automation, and text generation.\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33m• Integrate AI models into web and cloud-based applications.\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m• Work with APIs and SDKs from OpenAI, Meta (Llama), and Falcon to build scalable AI solutions.\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33m• Perform data preprocessing, model training, and performance evaluation.\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m• Stay up to date with the latest advancements in natural language processing (NLP) and LLMs.\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m• Collaborate with cross-functional teams to deliver AI-driven projects.\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m• Troubleshoot and enhance model performance using hyperparameter tuning and optimization techniques.\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m• Ensure compliance with AI ethics, security, and data privacy standards.\u001b[39m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33mRequired Skills & Qualifications:\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[33m• Experience between 3-6 yrs in AI/ML development, with hands-on experience in Llama, OpenAI (GPT models), and Falcon.\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33m• Strong programming skills in Python, with experience in TensorFlow, PyTorch, or Hugging Face Transformers.\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[33m• Proficiency in API development and integration with AI models.\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[33m• Experience in fine-tuning large language models (LLMs) for various applications.\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[33m• Knowledge of cloud platforms (AWS, GCP, or Azure) for AI model deployment.\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[33m• Hands-on experience with NLP techniques, embeddings, and tokenization.\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[33m• Strong understanding of vector databases (e.g., Pinecone, FAISS) for efficient AI retrieval tasks.\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[33m• Experience in optimizing AI models for speed, accuracy, and scalability.\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[33m• Familiarity with Docker and Kubernetes for deploying AI applications.\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[33m• Strong problem-solving skills and ability to work in an agile development environment.\u001b[39m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[33mPreferred Qualifications:\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33m• Experience working with LangChain and RAG-based (Retrieval-Augmented Generation) AI models.\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[33m• Knowledge of MLOps practices for model lifecycle management.\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[33m• Experience with Graph Neural Networks (GNNs) or multimodal AI models.\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33m• Background in AI security and bias mitigation.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     37\u001b[39m     },\n\u001b[32m     38\u001b[39m     config=config,\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# stream_mode=\"debug\",\u001b[39;00m\n\u001b[32m     40\u001b[39m     debug = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     41\u001b[39m ):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(update)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     44\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m----------------------------------------------------------------------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2305\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[39m\n\u001b[32m   2299\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2300\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2301\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2302\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2303\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2304\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2305\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2306\u001b[39m         loop.tasks.values(),\n\u001b[32m   2307\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2308\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2309\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2310\u001b[39m     ):\n\u001b[32m   2311\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2312\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2313\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langgraph/pregel/runner.py:444\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    442\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    445\u001b[39m         t,\n\u001b[32m    446\u001b[39m         retry_policy,\n\u001b[32m    447\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    448\u001b[39m         configurable={\n\u001b[32m    449\u001b[39m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[32m    450\u001b[39m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[32m    451\u001b[39m         },\n\u001b[32m    452\u001b[39m     )\n\u001b[32m    453\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langgraph/pregel/retry.py:128\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, configurable)\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:583\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    579\u001b[39m config = patch_config(\n\u001b[32m    580\u001b[39m     config, callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    581\u001b[39m )\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m    584\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    585\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langgraph/utils/runnable.py:371\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[32m    370\u001b[39m     coro = cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs))\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(coro, context=context)\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    373\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mresume_parser_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresume_parser_node\u001b[39m(state: Interview):\n\u001b[32m      6\u001b[39m     resume = parse_pdf(\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m../media/resume.pdf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )  \u001b[38;5;66;03m# TODO: we can make it a task using @task in the node ig.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     parsed_resume = \u001b[38;5;28;01mawait\u001b[39;00m resume_breaker.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m: resume})\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexperiences\u001b[39m\u001b[33m\"\u001b[39m: parsed_resume.experiences,\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprojects\u001b[39m\u001b[33m\"\u001b[39m: parsed_resume.projects,\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mskills\u001b[39m\u001b[33m\"\u001b[39m: parsed_resume.skills,\n\u001b[32m     14\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3071\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3069\u001b[39m     part = functools.partial(step.ainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   3070\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(part(), context=context)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   3072\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3073\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(part())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5377\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5371\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5372\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5373\u001b[39m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[32m   5374\u001b[39m     config: Optional[RunnableConfig] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5375\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5376\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5378\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5379\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5380\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5381\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:328\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    321\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     **kwargs: Any,\n\u001b[32m    326\u001b[39m ) -> BaseMessage:\n\u001b[32m    327\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    329\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    330\u001b[39m         stop=stop,\n\u001b[32m    331\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    332\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    333\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    334\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    335\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    336\u001b[39m         **kwargs,\n\u001b[32m    337\u001b[39m     )\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:853\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    846\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    847\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    850\u001b[39m     **kwargs: Any,\n\u001b[32m    851\u001b[39m ) -> LLMResult:\n\u001b[32m    852\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    854\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    855\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:813\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    800\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    802\u001b[39m             *[\n\u001b[32m    803\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m             ]\n\u001b[32m    812\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    814\u001b[39m flattened_outputs = [\n\u001b[32m    815\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    816\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    817\u001b[39m ]\n\u001b[32m    818\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:981\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    980\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m    982\u001b[39m             messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m    983\u001b[39m         )\n\u001b[32m    984\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    985\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/langchain_groq/chat_models.py:524\u001b[39m, in \u001b[36mChatGroq._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    519\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    520\u001b[39m params = {\n\u001b[32m    521\u001b[39m     **params,\n\u001b[32m    522\u001b[39m     **kwargs,\n\u001b[32m    523\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(messages=message_dicts, **params)\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/groq/resources/chat/completions.py:649\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    494\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    495\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    525\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    526\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m    527\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    529\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    647\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/openai/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    651\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    652\u001b[39m             {\n\u001b[32m    653\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m    654\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m    655\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m    656\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m    657\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m    658\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m    659\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m    660\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m    661\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m    662\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m    663\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m    664\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m    665\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_format\u001b[39m\u001b[33m\"\u001b[39m: reasoning_format,\n\u001b[32m    666\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m    667\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m    668\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m    669\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m    670\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m    671\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m    672\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m    673\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m    674\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m    675\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m    676\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m    677\u001b[39m             },\n\u001b[32m    678\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m    679\u001b[39m         ),\n\u001b[32m    680\u001b[39m         options=make_request_options(\n\u001b[32m    681\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    682\u001b[39m         ),\n\u001b[32m    683\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m    684\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    685\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m    686\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/groq/_base_client.py:1818\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1804\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1806\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1813\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1814\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1815\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1816\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1817\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/groq/_base_client.py:1526\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1524\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1526\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1527\u001b[39m     cast_to=cast_to,\n\u001b[32m   1528\u001b[39m     options=options,\n\u001b[32m   1529\u001b[39m     stream=stream,\n\u001b[32m   1530\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1531\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1532\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/interviewer/.venv/lib/python3.13/site-packages/groq/_base_client.py:1627\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1624\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1626\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1630\u001b[39m     cast_to=cast_to,\n\u001b[32m   1631\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1635\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1636\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
      "During task with name 'resume_parser_node' and id '6796ae46-84c7-ab56-7a5e-9a28a87389eb'"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"aayush1\"}}\n",
    "\n",
    "async for update in graph.astream(\n",
    "    {\n",
    "        \"job_description\": \"\"\"\n",
    "We are seeking a highly skilled AI Developer with expertise in Llama, OpenAI, and Falcon models to join our team. The ideal candidate will have strong experience in developing, fine-tuning, and deploying AI models, working with large language models (LLMs), and integrating AI solutions into real-world applications.\n",
    "\n",
    "Key Responsibilities:\n",
    "• Develop, fine-tune, and deploy AI/ML models using Llama, OpenAI, and Falcon.\n",
    "• Optimize and customize pre-trained language models for specific use cases.\n",
    "• Implement AI-powered solutions for chatbots, automation, and text generation.\n",
    "• Integrate AI models into web and cloud-based applications.\n",
    "• Work with APIs and SDKs from OpenAI, Meta (Llama), and Falcon to build scalable AI solutions.\n",
    "• Perform data preprocessing, model training, and performance evaluation.\n",
    "• Stay up to date with the latest advancements in natural language processing (NLP) and LLMs.\n",
    "• Collaborate with cross-functional teams to deliver AI-driven projects.\n",
    "• Troubleshoot and enhance model performance using hyperparameter tuning and optimization techniques.\n",
    "• Ensure compliance with AI ethics, security, and data privacy standards.\n",
    "\n",
    "Required Skills & Qualifications:\n",
    "• Experience between 3-6 yrs in AI/ML development, with hands-on experience in Llama, OpenAI (GPT models), and Falcon.\n",
    "• Strong programming skills in Python, with experience in TensorFlow, PyTorch, or Hugging Face Transformers.\n",
    "• Proficiency in API development and integration with AI models.\n",
    "• Experience in fine-tuning large language models (LLMs) for various applications.\n",
    "• Knowledge of cloud platforms (AWS, GCP, or Azure) for AI model deployment.\n",
    "• Hands-on experience with NLP techniques, embeddings, and tokenization.\n",
    "• Strong understanding of vector databases (e.g., Pinecone, FAISS) for efficient AI retrieval tasks.\n",
    "• Experience in optimizing AI models for speed, accuracy, and scalability.\n",
    "• Familiarity with Docker and Kubernetes for deploying AI applications.\n",
    "• Strong problem-solving skills and ability to work in an agile development environment.\n",
    "\n",
    "Preferred Qualifications:\n",
    "• Experience working with LangChain and RAG-based (Retrieval-Augmented Generation) AI models.\n",
    "• Knowledge of MLOps practices for model lifecycle management.\n",
    "• Experience with Graph Neural Networks (GNNs) or multimodal AI models.\n",
    "• Background in AI security and bias mitigation.\"\"\"\n",
    "    },\n",
    "    config=config,\n",
    "    # stream_mode=\"debug\",\n",
    "    debug = True\n",
    "):\n",
    "    print(update)\n",
    "    print(\n",
    "        \"----------------------------------------------------------------------------------------------------------------\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in graph.astream(\n",
    "    Command(resume=\"I built a web scraper using selenium that scraped data from jobscan and i used the generated data to to tailor the resume for the job i was applying for.\"),\n",
    "    # \"I built a web scraper using selenium that scraped data from jobscan and i used the generated data to to tailor the resume for the job i was applying for.\",\n",
    "    config=config,\n",
    "    # stream_mode=\"debug\",\n",
    "):\n",
    "    print(event)\n",
    "    print(\n",
    "        \"----------------------------------------------------------------------------------------------------------------\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
